import torch
import pandas as pd
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
from sklearn.metrics import accuracy_score

# --- Configuration ---
# Your base Llama 7B model ID (use the correct one you fine-tuned from)
BASE_MODEL_ID = "/SI425/Llama-2-7b-hf" 
# The local path where your LoRA adapter weights are saved
LORA_ADAPTER_PATH = "./ID_lora/checkpoint-4132" 
# Path to your dataset 
DATA_PATH = "./authorID.tsv" 
# Name of the column containing the passage text in your dataset
TEXT_COLUMN = "formatted_prompt" 
# Name of the column containing the actual author (ground truth)
AUTHOR_COLUMN = "author" 
# The template to structure the prompt for the model, not needed in this case, prompt already in tsv
PROMPT_TEMPLATE = (
    "Identify the author of the following passage. Respond with ONLY the author's name.\n\n"
    "Passage: {passage}\n\n"
    "Author:"
)
# Maximum number of tokens the model is allowed to generate for the prediction
MAX_NEW_TOKENS = 40 
# You can set this to True if you're using a system with high VRAM or a compatible 8-bit setup
LOAD_IN_8BIT = False 
# --- End Configuration ---

def load_model_and_tokenizer(base_model_id, lora_adapter_path, load_in_8bit=False):
    """Loads the base model, applies the LoRA adapter, and loads the tokenizer."""
    print("Loading base model and tokenizer...")
    
    # Configuration for memory-efficient loading
    device_map = {"": 0} if torch.cuda.is_available() else "auto"
    
    # Load Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model_id)
    
    # Define quantization settings (4-bit) to reduce model size when loaded.
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,                  # enable 4-bit quantization
        bnb_4bit_quant_type="nf4",          # use "normal float 4" quantization
        bnb_4bit_compute_dtype=torch.float16,  # compute in half precision
    )

    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto", # Automatically uses CUDA if available
        trust_remote_code=True
    )


    # Load LoRA Adapter
    model = PeftModel.from_pretrained(model, lora_adapter_path)
    
    # For inference, merge the LoRA weights into the base model weights
    # This is often done to improve inference speed
    model = model.merge_and_unload()
    model.eval() # Set model to evaluation mode

    print("Model and adapter loaded successfully.")
    return model, tokenizer

def generate_prediction(model, tokenizer, prompt, max_new_tokens=10):
    """Generates the author prediction from the model for a single passage."""
    
    # 2. Tokenize the input prompt
    inputs = tokenizer(prompt, return_tensors="pt")
    if torch.cuda.is_available():
        inputs = {name: tensor.to("cuda") for name, tensor in inputs.items()}

    # 3. Generate the response
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            pad_token_id=tokenizer.eos_token_id,
            do_sample=False,  # Use greedy decoding for consistent predictions
        )
    
    # 4. Decode the generated tokens
    # We only want the *new* tokens generated by the model, not the input prompt
    output_text = tokenizer.decode(
        outputs[0][len(inputs["input_ids"][0]):], 
        skip_special_tokens=True
    ).strip()
    
    print( output_text.split('\n') )# Take only the first line

if __name__ == "__main__":
    """Main function to run the evaluation process."""
    try:
        # 1. Load Data
        df = pd.read_csv(DATA_PATH, sep='\t')
        print(f"Loaded {len(df)} passages from {DATA_PATH}.")
        
        # 2. Load Model
        model, tokenizer = load_model_and_tokenizer(
            BASE_MODEL_ID, LORA_ADAPTER_PATH, LOAD_IN_8BIT
        )
        
        predictions = []
        # 3. Iterate and Predict
        for index, row in df.iterrows():
            prompt = row[TEXT_COLUMN]
            
            # Predict the author
            #predicted_author = 
            generate_prediction(
                model, tokenizer, prompt, MAX_NEW_TOKENS
            )
            
            #predictions.append(predicted_author)
            
            # Optional: Print progress for a few samples
            #if index < 5 or index % 1000 == 0:
                #print(f"\n--- Sample {index} ---")
                #print(f"Passage: {prompt[:100]}...")
                #print(f"Actual Author: {row[AUTHOR_COLUMN]}")
                #print(f"Predicted Author: {predicted_author}")

        exit()
        # 4. Add predictions to DataFrame and Save
        df['predicted_author'] = predictions
        output_file = "author_identification_results.tsv"
        df.to_tsv(output_file, index=False)
        print(f"\nâœ… All predictions saved to {output_file}")

        # 5. Evaluate Performance
        actual_authors = df[AUTHOR_COLUMN].str.strip().str.lower()
        predicted_authors = df['predicted_author'].str.strip().str.lower()
        
        # Calculate Exact Match Accuracy
        accuracy = accuracy_score(actual_authors, predicted_authors)
        
        print("\n--- Evaluation Results ---")
        print(f"Total Passages: {len(df)}")
        print(f"Exact Match Accuracy: {accuracy:.4f}")
        
    except FileNotFoundError:
        print(f"ðŸš¨ Error: Could not find data file at {DATA_PATH} or model/adapter path.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
